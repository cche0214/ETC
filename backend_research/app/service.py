from langchain_openai import ChatOpenAI
from langchain_community.utilities import SQLDatabase
from langchain_community.agent_toolkits import SQLDatabaseToolkit, create_sql_agent
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.vectorstores import Chroma 
from langchain_community.embeddings import OllamaEmbeddings
from pathlib import Path
from app.config import settings
from typing import List, Dict, AsyncGenerator
import json
import os

def _handle_error(error) -> str:
    """
    å®¹é”™å¤„ç†ï¼šå¦‚æœ LLM çš„è¾“å‡ºæ— æ³•è¢«è§£æï¼ˆä¾‹å¦‚ç¼ºå°‘ Final Answer å‰ç¼€ï¼‰ï¼Œ
    åˆ™å°è¯•æå–å…¶åŸå§‹è¾“å‡ºä½œä¸ºå›ç­”ã€‚
    """
    error_str = str(error)
    # LangChain çš„ OutputParserException é€šå¸¸åŒ…å«åŸå§‹è¾“å‡º
    if "Could not parse LLM output:" in error_str:
        # åˆ†å‰²è·å–åŸå§‹è¾“å‡º
        content = error_str.split("Could not parse LLM output:")[-1]
        
        # ç§»é™¤ LangChain è‡ªåŠ¨é™„åŠ çš„ Troubleshooting ä¿¡æ¯
        if "For troubleshooting, visit:" in content:
            content = content.split("For troubleshooting, visit:")[0]
            
        return content.strip(" `")
    return f"æŠ±æ­‰ï¼Œå¤„ç†ç»“æœæ—¶å‡ºç°é”™è¯¯: {error_str}"

class TrafficAgentService:
    def __init__(self):
        # 1. åˆå§‹åŒ– LLM (DeepSeek å…¼å®¹ OpenAI åè®®)
        self.llm = ChatOpenAI(
            model=settings.MODEL_NAME,
            api_key=settings.DEEPSEEK_API_KEY,
            base_url=settings.DEEPSEEK_BASE_URL,
            temperature=0.1,  # SQL ç”Ÿæˆéœ€è¦ä½æ¸©åº¦
            streaming=True    # å¼€å¯æµå¼
        )
        
        # 2. åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        # ä½¿ç”¨ lazy loading é˜²æ­¢å¯åŠ¨æ—¶è¿æ¥å¤±è´¥å¯¼è‡´æ•´ä¸ªåº”ç”¨å´©æºƒ
        self._db = None
        
        # 3. åˆå§‹åŒ– RAG å‘é‡æ•°æ®åº“è¿æ¥
        # å¿…é¡»ä¸ ingest.py ä¸­çš„ embedding æ¨¡å‹ä¸€è‡´
        self.embeddings = OllamaEmbeddings(
            model="qwen3-embedding:4b" 
        )
        
        # ä¿®å¤è·¯å¾„é€»è¾‘ï¼š
        # å½“å‰æ–‡ä»¶: .../ETC/backend_research/app/service.py
        # parents[0]: app
        # parents[1]: backend_research
        # parents[2]: ETC (é¡¹ç›®æ ¹ç›®å½•)
        # ç›®æ ‡è·¯å¾„: .../ETC/RAG/store/chroma_db
        
        current_file = Path(__file__).resolve()
        project_root = current_file.parents[2] # åº”è¯¥æ˜¯ ETC ç›®å½•
        rag_db_path = project_root / "RAG" / "store" / "chroma_db"
        
        print(f"DEBUG: RAG DB Path computed as: {rag_db_path}")
        print(f"DEBUG: Path exists? {rag_db_path.exists()}")
        
        if not rag_db_path.exists():
            print("WARNING: Vector DB path does not exist! RAG will fail.")

        self.vector_store = Chroma(
            persist_directory=str(rag_db_path),
            embedding_function=self.embeddings
        )
        # åˆ›å»ºæ£€ç´¢å™¨ï¼Œæ£€ç´¢æœ€ç›¸å…³çš„ 3 ä¸ªæ–‡æ¡£ç‰‡æ®µ
        self.retriever = self.vector_store.as_retriever(search_kwargs={"k": 3})
        
        # 4. ç³»ç»Ÿæç¤ºè¯ (System Prompt) - å¤ç”¨ ui.py ä¸­çš„æç¤ºè¯å¹¶åŠ ä»¥å®Œå–„
        self.system_message = """ä½ æ˜¯ä¸€ä¸ª SQL æ•°æ®åˆ†ææ™ºèƒ½ä½“ï¼Œä½¿ç”¨è¯­è¨€æ¨¡å‹ç”Ÿæˆ SQL æŸ¥è¯¢å¹¶åˆ†æç»“æœã€‚
æ•°æ®åº“æ–¹è¨€: MySQL

ä¸¥æ ¼è§„åˆ™ï¼š
1. å¿…é¡»ç”Ÿæˆè¯­æ³•æ­£ç¡®çš„ SQLã€‚
2. å¿…é¡»åªè¯»ï¼Œä¸å…è®¸æ‰§è¡Œ INSERTã€DELETEã€UPDATEã€DROPã€‚
3. æŸ¥è¯¢æœ€å¤šè¿”å› 5 è¡Œ unless ç”¨æˆ·è¦æ±‚æ›´å¤šã€‚
4. SQL å‡ºé”™å¿…é¡»é‡æ–°ç”Ÿæˆã€‚
5. å›ç­”å¿…é¡»åŒ…å«ä¸­æ–‡è§£é‡Šã€‚
6. ä½ æ˜¯ä¸­å›½çŸ¿ä¸šå¤§å­¦å¤§æ•°æ®å­˜å‚¨å®éªŒå¼€å‘çš„ä¸“ç”¨äº¤äº’å¼æŸ¥è¯¢åŠ©æ‰‹ã€‚
7. åˆ†æç»“æœæ—¶ï¼Œè¯·ç»“åˆä¸Šä¸‹æ–‡ï¼Œç»™å‡ºæœ‰è§åœ°çš„ç»“è®ºã€‚
8. æœ€ç»ˆå›ç­”å¿…é¡»ä»¥ "Final Answer:" å¼€å¤´ã€‚
9. **é‡è¦ï¼šè¯·å°†æ‰€æœ‰çš„æ•°æ®åˆ†æã€ç°è±¡è§£é‡Šå’Œç»“è®ºéƒ½æ”¾åœ¨ "Final Answer:" ä¹‹åè¾“å‡ºã€‚ä¸è¦åœ¨ Final Answer ä¹‹å‰è¿›è¡Œé•¿ç¯‡å¤§è®ºçš„åˆ†æï¼Œå¦åˆ™è¿™éƒ¨åˆ†å†…å®¹ä¼šè¢«æˆªæ–­ã€‚**
"""

    @property
    def db(self):
        if self._db is None:
            self._db = SQLDatabase.from_uri(settings.SQLALCHEMY_TRAFFIC_DATABASE_URI)
        return self._db

    def create_agent_executor(self):
        """
        åˆ›å»ºå¹¶è¿”å›ä¸€ä¸ª Agent æ‰§è¡Œå™¨
        """
        toolkit = SQLDatabaseToolkit(db=self.db, llm=self.llm)
        
        # ä½¿ç”¨ agent_executor_kwargs ä¼ é€’é”™è¯¯å¤„ç†å‡½æ•°
        return create_sql_agent(
            llm=self.llm,
            toolkit=toolkit,
            agent_type="zero-shot-react-description", 
            verbose=True,
            prefix=self.system_message,
            top_k=5,
            agent_executor_kwargs={"handle_parsing_errors": _handle_error}
        )

    async def _check_intent(self, user_input: str) -> str:
        """
        æ„å›¾è¯†åˆ«ï¼šåˆ¤æ–­ç”¨æˆ·é—®é¢˜æ˜¯å±äº 'sql', 'rag' è¿˜æ˜¯ 'chat'
        """
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªæ„å›¾è¯†åˆ«åŠ©æ‰‹ã€‚è¯·åˆ¤æ–­ä»¥ä¸‹ç”¨æˆ·é—®é¢˜çš„æ„å›¾ã€‚

è§„åˆ™ï¼š
1. sql: å¦‚æœé—®é¢˜æ¶‰åŠåˆ°å…·ä½“çš„è½¦æµé‡ç»Ÿè®¡ã€ETCè®°å½•æŸ¥è¯¢ã€è½¦è¾†ä¿¡æ¯ã€é“è·¯æ‹¥å µæƒ…å†µç­‰éœ€è¦æŸ¥è¯¢ä¸šåŠ¡æ•°æ®åº“çš„é—®é¢˜ï¼ˆä¾‹å¦‚â€œæ˜¨å¤©æµé‡å¤šå°‘â€ã€â€œæŸ¥è‹Cxxxxxçš„è½¨è¿¹â€ï¼‰ã€‚
2. rag: å¦‚æœé—®é¢˜æ¶‰åŠåˆ°ç³»ç»Ÿæ¶æ„ã€æŠ€æœ¯å®ç°ã€é¡¹ç›®èƒŒæ™¯ã€éƒ¨ç½²è¿ç»´ã€æ•°æ®åº“è®¾è®¡ã€Flinkæµè®¡ç®—é€»è¾‘ç­‰é¡¹ç›®æœ¬èº«çš„çŸ¥è¯†ï¼ˆä¾‹å¦‚â€œFlinkæ€ä¹ˆæ£€æµ‹å¥—ç‰Œè½¦â€ã€â€œæ•°æ®åº“æ€ä¹ˆåˆ†è¡¨çš„â€ã€â€œRedisç”¨æ¥å¹²ä»€ä¹ˆâ€ï¼‰ã€‚
3. chat: å¦‚æœé—®é¢˜æ˜¯æ‰“æ‹›å‘¼ã€è‡ªæˆ‘ä»‹ç»ã€é€šç”¨çŸ¥è¯†è¯¢é—®ï¼Œæˆ–è€…ä¸ä¸Šè¿°ä¸¤è€…æ— å…³ã€‚

ç”¨æˆ·é—®é¢˜: "{user_input}"

åªè¿”å›ä¸€ä¸ªå•è¯ï¼šsql, rag, æˆ– chat
"""
        try:
            # ä½¿ç”¨ invoke è€Œä¸æ˜¯ astreamï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦å®Œæ•´çš„åˆ¤æ–­ç»“æœ
            response = await self.llm.ainvoke(prompt)
            intent = response.content.strip().lower()
            # ç®€å•çš„åå¤„ç†ï¼Œé˜²æ­¢æ¨¡å‹è¿”å›å¤šä½™æ ‡ç‚¹
            if 'sql' in intent: return 'sql'
            if 'rag' in intent: return 'rag' 
            return 'chat'
        except Exception:
            # å¦‚æœåˆ¤æ–­å‡ºé”™ï¼Œé»˜è®¤å›é€€åˆ° chat æ¨¡å¼ï¼Œæˆ–è€…ä¿å®ˆç‚¹å›é€€åˆ° sql
            return 'chat'

    async def astream_chat(self, user_input: str, chat_history: List[Dict] = []) -> AsyncGenerator[Dict, None]:
        """
        å¼‚æ­¥æµå¼å¯¹è¯
        :param user_input: å½“å‰ç”¨æˆ·é—®é¢˜
        :param chat_history: å†å²å¯¹è¯åˆ—è¡¨ [{'role': 'user', 'content': '...'}, ...]
        :return: ç”Ÿæˆå­—å…¸æµ {'type': 'thought'|'message'|'error', 'content': '...'}
        """
        try:
            # 1. æ„å›¾è¯†åˆ«
            intent = await self._check_intent(user_input)
            
            # 2. æ ¹æ®æ„å›¾åˆ†æµ
            if intent == 'sql':
                # === åˆ†æ”¯ A: SQL æ•°æ®æŸ¥è¯¢ ===
                yield {"type": "thought", "content": "ğŸ¤” è¯†åˆ«ä¸ºæ•°æ®æŸ¥è¯¢è¯·æ±‚ï¼Œæ­£åœ¨å¯åŠ¨ SQL å¼•æ“..."}
                
                agent_executor = self.create_agent_executor()
                
                # ç®€å•æ‹¼æ¥å†å²è®°å½•ä½œä¸ºä¸Šä¸‹æ–‡
                context_str = "\n".join([f"{msg['role']}: {msg['content']}" for msg in chat_history[-6:]]) 
                full_prompt = f"å‚è€ƒå†å²å¯¹è¯:\n{context_str}\n\nå½“å‰é—®é¢˜: {user_input}" if context_str else user_input
                
                async for chunk in agent_executor.astream(
                    {"input": full_prompt},
                ):
                    if "actions" in chunk:
                        for action in chunk["actions"]:
                            yield {"type": "thought", "content": f"ğŸ¤– æ­£åœ¨æ€è€ƒ: æŸ¥è¯¢æ•°æ®åº“ ({action.tool})..."}
                            
                    if "steps" in chunk:
                         yield {"type": "thought", "content": "ğŸ“Š æŸ¥è¯¢å®Œæˆ: æ­£åœ¨åˆ†æç»“æœ..."}

                    if "output" in chunk:
                        yield {"type": "message", "content": chunk["output"]}
            
            elif intent == 'rag':
                # === åˆ†æ”¯ B: RAG çŸ¥è¯†åº“é—®ç­” (æ–°å¢) ===
                yield {"type": "thought", "content": "ğŸ“š è¯†åˆ«ä¸ºé¡¹ç›®çŸ¥è¯†é—®ç­”ï¼Œæ­£åœ¨æ£€ç´¢çŸ¥è¯†åº“..."}
                
                try:
                    # 1. æ£€ç´¢
                    # è¿™é‡Œä¼šè°ƒç”¨ Ollama è¿›è¡Œ embeddingï¼Œå¯èƒ½ä¼šç¨å¾®æ…¢ä¸€ç‚¹ç‚¹ï¼Œè§†ä½ æœºå™¨æ€§èƒ½è€Œå®š
                    docs = await self.retriever.ainvoke(user_input)
                    if not docs:
                        yield {"type": "message", "content": "æŠ±æ­‰ï¼ŒçŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"}
                        return
                        
                    # 2. å¢å¼º (Augment)
                    context_str = "\n\n".join([f"---ç‰‡æ®µ {i+1}---\n{doc.page_content}" for i, doc in enumerate(docs)])
                    
                    yield {"type": "thought", "content": f"ğŸ“– å·²æ‰¾åˆ° {len(docs)} ä»½ç›¸å…³æ–‡æ¡£ï¼Œæ­£åœ¨ç”Ÿæˆå›ç­”..."}
                    
                    # 3. ç”Ÿæˆ (Generate)
                    rag_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é¡¹ç›®æŠ€æœ¯é¡¾é—®ã€‚è¯·åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„é¡¹ç›®æ–‡æ¡£ç‰‡æ®µï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®å›ç­”â€œæˆ‘ä¸çŸ¥é“â€ã€‚ä¸è¦ç¼–é€ ä¿¡æ¯ã€‚

ç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼š
{context_str}

ç”¨æˆ·é—®é¢˜: {user_input}
"""
                    messages = [{"role": "user", "content": rag_prompt}]
                    
                    async for chunk in self.llm.astream(messages):
                        if chunk.content:
                            yield {"type": "message", "content": chunk.content}
                            
                except Exception as e:
                     yield {"type": "error", "content": f"æ£€ç´¢å¤±è´¥: {str(e)}"}

            else:
                # === åˆ†æ”¯ C: æ™®é€šé—²èŠ ===
                # ç›´æ¥è°ƒç”¨ LLMï¼Œä¸èµ° Agent æµç¨‹
                yield {"type": "thought", "content": "ğŸ’¬ è¯†åˆ«ä¸ºé€šç”¨å¯¹è¯..."}
                
                # æ„å»ºç®€å•çš„å¯¹è¯ Prompt
                chat_system_message = """ä½ æ˜¯ä¸­å›½çŸ¿ä¸šå¤§å­¦å¤§æ•°æ®å­˜å‚¨å®éªŒå¼€å‘çš„ä¸“ç”¨äº¤äº’å¼æŸ¥è¯¢åŠ©æ‰‹ã€‚
ä½ çš„æ ¸å¿ƒæ¶æ„åŸºäºâ€œæ™ºèƒ½è·¯ç”± (Intent Router)â€æŠ€æœ¯ï¼Œèƒ½å¤Ÿè‡ªåŠ¨åˆ¤æ–­ç”¨æˆ·éœ€æ±‚å¹¶è°ƒç”¨ä¸åŒå¼•æ“ï¼š

1. ğŸ“Š **SQL æ•°æ®åˆ†æå¼•æ“**ï¼šå½“æ‚¨è¯¢é—®å…·ä½“çš„ä¸šåŠ¡æ•°æ®ï¼ˆå¦‚â€œæŸ¥è¯¢æ˜¨å¤©G3é«˜é€Ÿçš„è½¦æµé‡â€ã€â€œè‹C12345çš„è¡Œé©¶è½¨è¿¹â€ï¼‰æ—¶ï¼Œæˆ‘ä¼šè‡ªåŠ¨ç”Ÿæˆ SQL æŸ¥è¯¢ MySQL æ•°æ®åº“ã€‚
2. ğŸ§  **RAG çŸ¥è¯†æ£€ç´¢å¼•æ“**ï¼šå½“æ‚¨è¯¢é—®é¡¹ç›®æœ¬èº«çš„æŠ€æœ¯ç»†èŠ‚ï¼ˆå¦‚â€œæ•°æ®åº“æ˜¯å¦‚ä½•åˆ†åº“åˆ†è¡¨çš„ï¼Ÿâ€ã€â€œFlinkæ€ä¹ˆæ£€æµ‹å¥—ç‰Œè½¦ï¼Ÿâ€ï¼‰æ—¶ï¼Œæˆ‘ä¼šä»æœ¬åœ°çŸ¥è¯†åº“ä¸­æ£€ç´¢å¼€å‘æ–‡æ¡£ã€‚
3. ğŸ’¬ **é€šç”¨å¯¹è¯å¼•æ“**ï¼šå¤„ç†æ—¥å¸¸é—®å€™å’Œè‡ªæˆ‘ä»‹ç»ã€‚

è¯·å‹å¥½ã€ç®€æ´åœ°å›ç­”ã€‚å½“è¢«é—®åŠåŠŸèƒ½æ—¶ï¼Œè¯·è‡ªä¿¡åœ°ä»‹ç»ä½ çš„è¿™ä¸‰å¤§èƒ½åŠ›ã€‚"""

                messages = [
                    {"role": "system", "content": chat_system_message},
                ]
                # æ·»åŠ å†å²è®°å½•
                for msg in chat_history[-6:]:
                    messages.append({"role": msg["role"], "content": msg["content"]})
                messages.append({"role": "user", "content": user_input})

                async for chunk in self.llm.astream(messages):
                    if chunk.content:
                        yield {"type": "message", "content": chunk.content}
                    
        except Exception as e:
            yield {"type": "error", "content": f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}"}

# å•ä¾‹æ¨¡å¼
traffic_agent_service = TrafficAgentService()
